---
title: "Fst"
author: "Claudius"
date: "17/12/2016"
output: 
  github_document:
    toc: yes
#  html_document:
#    self_contained: true
#    theme: cosmo
#    toc: true
bibliography: Literature.bib
---

```{r setup options, cache=FALSE, include=FALSE}
library(knitr)
opts_chunk$set(dev=c("png", "pdf"))
setwd("/data3/claudius/Big_Data/ANGSD/FST")
```

# Unfolded SAF's

I have created unfolded SAF's and used an unfolded 2D-SFS to calculate posterior expectations of per-site $F_{ST}$. `realSFS` allows the calculation from two different formula's. One according to Reynolds [@Fumagalli2013] and the other accroding to Hudson/Bhatia [@Bhatia2013].

```{r, cache=TRUE}
bhatia = read.delim("EryPar.Bhatia.fst.tab", header=F)
names(bhatia) = c("contig", "pos", "Hb.minus.Hw", "Hb") 
nrow(bhatia)
str(bhatia)
save(bhatia, file="bhatia.RData")
reynolds = read.delim("EryPar.Reynolds.fst.tab", header=F)
names(reynolds) = c("contig", "pos", "a", "a.plus.b")
head(reynolds)
```


## Global average $F_{ST}$ between ERY and PAR

From the per-site denominators and numerators of the two formulas, which are reported in columns 3 and 4 respectively, I want to calculate the global average $F_{ST}$ by taking the "ratio of averages" as suggested by [@Bhatia2013] (instead of the average of per-site $F_{ST}$ estimates).

```{r}
( Fst.bhatia.global = sum(bhatia[["Hb.minus.Hw"]])/sum(bhatia[["Hb"]]) )
nrow(bhatia)
( Fst.reynolds.global = sum(reynolds$a)/sum(reynolds[["a.plus.b"]]) )
nrow(reynolds)
```

Both formulas provide very similar estimates. The estimates are taken over `r nrow(bhatia)` sites. Reynolds $F_{ST}$ is weighting by sample size, which I think is an undesirable property (i. e. a change in sample size, that does not change the estimate of sample allele frequencies can change the estimate of $F_{ST}$).


## $F_{ST}$ by contig

In the following, I want to collapse the data set, by keeping only the average $F_{ST}$ per contig. 

### Bhatia

```{r}
Fst.by.contig = aggregate(cbind(Hb.minus.Hw, Hb) ~ contig, 
                          data=bhatia, 
                          sum
                            )
Fst.by.contig = cbind(Fst.by.contig, FST=Fst.by.contig[,2]/Fst.by.contig[,3])
#
contig.lengths = tapply(bhatia$contig, bhatia$contig, length)
head(names(contig.lengths))
head(Fst.by.contig)
tail(names(contig.lengths))
tail(Fst.by.contig)
Fst.by.contig = cbind(Fst.by.contig, length=contig.lengths)
head(Fst.by.contig)
save(Fst.by.contig, file="Fst.by.contig.bhatia.RData")
#
( sum(Fst.by.contig$Hb.minus.Hw)/sum(Fst.by.contig$Hb) )
```

I have now average $F_{ST}$ estimates for `r nrow(Fst.by.contig)` contigs.


```{r bhatia-fst-by-contig-hist-unfolded}
hist(Fst.by.contig$FST, xlab=expression(paste("average Bhatia's ", F[ST])), 
     breaks=100,
     main=bquote(paste(F[ST], "'s from ", .(nrow(Fst.by.contig)), " contigs")),
     col="black",
     # xlim=c(0, .3),
     border="black"
     )
# proportion of loci with Fst greater than average Fst:
gtAvgFst = sum(Fst.by.contig$FST > Fst.bhatia.global)/length(Fst.by.contig$FST)
```

As expected, the majority of the contigs are very little differentiated between the two populations, but `r signif(gtAvgFst*100, digits=1)`% of loci have $F_{ST}$ greater than the global average. Please read [@Whitlock2015]! It would be interesting to simulate neutral distributions of $F_{ST}$ under different demographic scenarios. The distribution might be overdispersed due to allele drop out [@Gautier2012].



#### Bootstrap 

It would be nice to get an estimate of uncertainty in the estimate of the global $F_{ST}$. There are two major sources of variation that influence this estimate:

1. sampling variation of individuals from populations
2. sampling variation of loci from the genome

In order to approximate the first source of variation, I could bootstrap resample individuals', but that requires many re-stimations of SAF's which will be quite compute and data intensive. I am therefore only going to approximate the second source of variation for the moment.

```{r bhatia-boot-global-Fst-unfolded, cache=TRUE}
library(dplyr) # for 'sample_n'
library(parallel) # for 'mclapply'
#
boot = function(x){
  # creates bootstrap resamples of the rows in Fst.by.contig
  # and returns the global Fst from that
  rs = sample_n(Fst.by.contig, size=nrow(Fst.by.contig), replace=TRUE)
  sum(rs$Hb.minus.Hw)/sum(rs$Hb)
  }
#
# serial:
#boot.resample.Fst.by.contig = replicate(10000, boot())
# parallel:
startTime = proc.time()
boot.resample.Fst.by.contig = vector("double", length=10000)
boot.resample.Fst.by.contig = simplify2array(
  mclapply(1:length(boot.resample.Fst.by.contig),
          FUN=boot,
          mc.cores=20
           )
  )
stopTime = proc.time()
elapsedTime = stopTime - startTime
show(elapsedTime)
# save bootstrap resample for later:
save(boot.resample.Fst.by.contig, file="Bootstrap/boot.resample.Fst.by.contig.RData")
#
d = density(boot.resample.Fst.by.contig)
plot(d,
     xlab=expression(paste("global ", F[ST])),
     main=bquote(paste(.(length(boot.resample.Fst.by.contig)), 
                       " bootstrap resamples of ", .(nrow(Fst.by.contig)), 
                       " contigs"
                       )
                 )
     )
points(c(Fst.bhatia.global), c(0), pch=3, cex=1.5)
CI95 = quantile(boot.resample.Fst.by.contig, probs=c(.025, .975))
lines(d$x[d$x>CI95[1] & d$x<CI95[2]], 
      d$y[d$x>CI95[1] & d$x<CI95[2]], 
      type="h",
      col="grey")
legend("topright", legend=c("real sample", "95% CI"), pch=c(3,15), pt.cex=1.5, col=c("black", "grey"), bty="n")
```


### Reynolds

```{r}
Fst.reynolds.by.contig = aggregate(cbind(a, a.plus.b) ~ contig, 
                          data=reynolds, 
                          sum
                            )
Fst.reynolds.by.contig = cbind(Fst.reynolds.by.contig, FST=Fst.reynolds.by.contig[,2]/Fst.reynolds.by.contig[,3])
head(Fst.reynolds.by.contig)
( sum(Fst.reynolds.by.contig$a)/sum(Fst.reynolds.by.contig$a.plus.b) )
```


```{r reynolds-fst-by-contig-hist-unfolded}
hist(Fst.reynolds.by.contig$FST, xlab=expression(paste("Reynolds ", F[ST])), 
     main=bquote(paste(F[ST], " over ", .(nrow(Fst.reynolds.by.contig)), " contigs")),
     col="grey"
     )
```


```{r}
boxplot(data.frame(Bhatia=Fst.by.contig$FST, Reynolds=Fst.reynolds.by.contig$FST), outline=FALSE, ylab=expression(F[ST]))
```

Reynolds' formula produces slightly higher $F_{ST}$ values.


### 2D-SFS

```{r}
sfs2d = scan("/data3/claudius/Big_Data/ANGSD/FST/EryPar.unfolded.2dsfs")
sfs2d = matrix(sfs2d, nrow=37, ncol=37) # rows should be PAR, columns should be ERY
sfs2d[1,1]
dim(sfs2d)
par_marginal_sfs = rowSums(sfs2d)
ery_marginal_sfs = colSums(sfs2d)
plot(1:(length(par_marginal_sfs)-1), par_marginal_sfs[2:length(par_marginal_sfs)], type="l")
sum(par_marginal_sfs)
# note, that the 'matrix is filled column-wise
sfs2d[1,1] = 0
# number of fixed differences:
fixed.diff = sfs2d[1,37] + sfs2d[37,1]
# proportion of fixed differences among polymorphic sites:
fixed.diff/sum(sfs2d)
```

```{r 2D-SFS-unfolded}
library(fields)
# rows in the matrix are on the x-axis, columns are on the y-axis:
image.plot(0:37, 0:37, sfs2d, xlab="PAR", ylab="ERY", main="unfolded 2D-SFS")
```


## Permutation test of global $F_{ST}$

I have randomly permutated the 36 individuals into a population 1 and population 2. I have then estimated SAF's, 2D-SFS's and $F_{ST}$ for each permutation.


```{r}
# only needs to be run once:
# fst.tabs = list.files("Bootstrap/", "*tab")
# perm = function(x){
#   bhatia = read.delim(fst.tabs[x], header=F)
#   return( sum(bhatia[,3])/sum(bhatia[,4]) )
# }
# startTime = proc.time()
# perm.fst = vector("double", len=length(fst.tabs))
# perm.fst = simplify2array(
#   mclapply(1:length(perm.fst),
#           FUN=perm,
#           mc.cores=20
#            )
#   )
# stopTime = proc.time()
# elapsedTime = stopTime - startTime
# show(elapsedTime)
# save(perm.fst, file="Bootstrapperm.global.fst.RData")
```

```{r permut-global-fst-hist}
load("Bootstrap/perm.global.fst.RData")
hist(boot.resample.Fst.by.contig, 
     border="navyblue", col="navyblue", 
     freq=FALSE, 
     xlim=c(0,.4),
     main="global average Fst\npermutation of individuals and bootstrapping of contigs",
     xlab=expression(paste("global Bhatia's ", F[ST]))
     )
hist(perm.fst, breaks=20, border="springgreen", col="springgreen", 
     add=TRUE,
     freq=FALSE
     )
legend("topleft", 
       legend=c("100 permutations of population label", 
                paste("10,000 bootstrap resamples of ", nrow(Fst.by.contig), " contigs")),
       col=c("springgreen", "navyblue"),
       cex=.7,
       fill=c("springgreen", "navyblue")
      )
```


## $F_{ST}$ by ascertainment class

I have calculated ML estimates of minor allele frequencies for each population, ERY and PAR. I have then created joint tables of each MAF file with `/data3/claudius/Big_Data/ANGSD/FST/EryPar.Bhatia.fst.tab`. I have now MAF's and $F_{ST}$ for 1,513,856 sites in ERY and
1,496,271 sites in PAR.

### Ascertainment in PAR

```{r}
pp = pipe("cut -f1,2,7,8,11,12 /data3/claudius/Big_Data/ANGSD/FST/MAFs/PAR/PAR.mafs.withFST", open="r")
par.mafs.withFST = read.delim(pp, header=F)
close(pp)
names(par.mafs.withFST) = c("contig", "pos", "MAF", "nind", "Hb.minus.Hw", "Hb")
head(par.mafs.withFST)
n = 18 # 18 individuals
global.fst.by.PAR.ascert = vector("double", n-1)
j = 0
for(i in 1:(n-1)){
  #print(i)
  j = j+1
  tab = par.mafs.withFST[par.mafs.withFST$MAF > i/(2*n) & par.mafs.withFST$MAF < (i+1)/(2*n),]
  global.fst.by.PAR.ascert[j] = sum(tab["Hb.minus.Hw"])/sum(tab["Hb"])
}
save(global.fst.by.PAR.ascert, file="global.fst.by.PAR.ascert.RData")
```


### Ascertainment in ERY

```{r}
pp = pipe("cut -f1,2,7,8,11,12 /data3/claudius/Big_Data/ANGSD/FST/MAFs/ERY/ERY.mafs.withFST", open="r")
ery.mafs.withFST = read.delim(pp, header=F)
close(pp)
names(ery.mafs.withFST) = c("contig", "pos", "MAF", "nind", "Hb.minus.Hw", "Hb")
head(ery.mafs.withFST)
n = 18 # 18 individuals
global.fst.by.ERY.ascert = vector("double", n-1)
j = 0
for(i in 1:(n-1)){
  #print(i)
  j = j+1
  tab = ery.mafs.withFST[ery.mafs.withFST$MAF > i/(2*n) & ery.mafs.withFST$MAF < (i+1)/(2*n),]
  global.fst.by.ERY.ascert[j] = sum(tab["Hb.minus.Hw"])/sum(tab["Hb"])
}
save(global.fst.by.ERY.ascert, file="global.fst.by.ERY.ascert.RData")
```

I am trying to produce a plot analogous to figure 1 in [@Bhatia2013].

```{r global-Fst-by-ascertainment}
plot(1:(n-1)/(2*n), global.fst.by.PAR.ascert, 
     ylim=c(0,.4), ylab=expression(paste("average Bhatia's ", F[ST])),
     xlab="minor allele frequency",
     pch=16, col="green",
     type="b",
     main="Allele frequency dependence of FST"
     )
lines(1:(n-1)/(2*n), global.fst.by.ERY.ascert, 
     ylim=c(0,1), ylab=expression(paste("average Bhatia's ", F[ST])),
     xlab="minor allele frequency",
     pch=16, col="red",
     type="b"
     )
legend("topright", 
       legend=c("ascertainment in PAR", "ascertainment in ERY", "without ascertainment"),
       col=c("green", "red", "grey"),
       fill=c("green", "red", "grey"),
       bty="n"
       )
abline(h=Fst.bhatia.global, lwd=2)
```


## $F_{ST}$ distributions


```{r}
# ascertaining by ML estimate of minor allele frequency:
ery.snps = ery.mafs.withFST[ery.mafs.withFST$MAF>1/(2*n),]
par.snps = par.mafs.withFST[par.mafs.withFST$MAF>1/(2*n),] # not edible
nrow(ery.snps)
nrow(par.snps)
head(ery.snps, n=20)
```

Parallelus seems to have more SNP's than erythropus.

### Ascertained in _erythropus_

```{r fst-dist-ascert-in-ery-Hw}
plot(-1*(ery.snps$Hb.minus.Hw-ery.snps$Hb), ery.snps$Hb.minus.Hw/ery.snps$Hb,
     xlab=expression(H[w]),
     ylab=expression(1-H[w]/H[b]),
     pch=16,
     col=gray(level=0, alpha=.3),
     main=expression(paste("Hudson/Bhatia's ", F[ST]))
    )
text(x=.15, y=.9, labels=paste(nrow(ery.snps), " SNP's ascertained in erythropus"), pos=4)
```

```{r fst-dist-ascert-in-ery-Hb}
plot(ery.snps$Hb, ery.snps$Hb.minus.Hw/ery.snps$Hb,
     xlab=expression(H[b]),
     ylab=expression(1-H[w]/H[b]),
     pch=16,
     col=gray(level=0, alpha=.3),
     main=expression(paste("Hudson/Bhatia's ", F[ST]))
    )
text(x=.05, y=.9, labels=paste(nrow(ery.snps), " SNP's ascertained in erythropus"), pos=4)
```

```{r fst-dist-ascert-in-ery-MAF1-9-Hb, fig.height=10, fig.width=10}
jpeg("fst-dist-ascert-in-ery-MAF1-9-Hb.jpg", width=1000, height=1000, quality=100)
par(mfrow=c(3,3))
for(i in 1:9){
# ascertainment in ery, no singletons
ery.snps = ery.mafs.withFST[ery.mafs.withFST$MAF>i/(2*n),]
nrow(ery.snps)
#
plot(ery.snps$Hb, ery.snps$Hb.minus.Hw/ery.snps$Hb,
     xlab=expression(H[b]),
     ylab=expression(1-H[w]/H[b]),
     pch=16,
     col=gray(level=0, alpha=.3),
     main=paste("MAF > ", i)
    )
text(x=.05, y=.9, labels=paste(nrow(ery.snps), " SNP's ascertained in erythropus"), pos=4)
}
dev.off()
```

### Ascertained in _parallelus_

```{r fst-dist-ascert-in-par-Hw}
plot(-1*(par.snps$Hb.minus.Hw-par.snps$Hb), par.snps$Hb.minus.Hw/par.snps$Hb,
     xlab=expression(H[w]),
     ylab=expression(1-H[w]/H[b]),
     pch=16,
     col=gray(level=0, alpha=.3),
     main=expression(paste("Hudson/Bhatia's ", F[ST]))
    )
text(x=.15, y=.9, labels=paste(nrow(par.snps), " SNP's ascertained in parallelus"), pos=4)
```

```{r fst-dist-ascert-in-par-Hb}
plot(par.snps$Hb, par.snps$Hb.minus.Hw/par.snps$Hb,
     xlab=expression(H[b]),
     ylab=expression(1-H[w]/H[b]),
     pch=16,
     col=gray(level=0, alpha=.3),
     main=expression(paste("Hudson/Bhatia's ", F[ST]))
    )
text(x=.05, y=.9, labels=paste(nrow(par.snps), " SNP's ascertained in parallelus"), pos=4)
```

Not sure what to make of these.




# Folded SAF's

see this [ngsTools thread](https://groups.google.com/d/topic/ngstools-user/9Z4viLJ7NJA/discussion)

I have also estimated *folded* SAF's and a folded 2D-SFS and estimated per-site $F_{ST}$ from that.

```{r, cache=TRUE}
bhatia = read.delim("EryPar.FOLDED.Bhatia.fst.tab", header=F)
names(bhatia) = c("contig", "pos", "Hb.minus.Hw", "Hb") 
reynolds = read.delim("EryPar.FOLDED.Reynolds.fst.tab", header=F)
names(reynolds) = c("contig", "pos", "a", "a.plus.b")
```

```{r}
( Fst.bhatia.global = sum(bhatia[["Hb.minus.Hw"]])/sum(bhatia[["Hb"]]) )
nrow(bhatia)
( Fst.reynolds.global = sum(reynolds$a)/sum(reynolds[["a.plus.b"]]) )
nrow(reynolds)
```

```{r}
Fst.by.contig = aggregate(cbind(Hb.minus.Hw, Hb) ~ contig, 
                          data=bhatia, 
                          sum
                            )
Fst.by.contig = cbind(Fst.by.contig, FST=Fst.by.contig[,2]/Fst.by.contig[,3])
head(Fst.by.contig)

( sum(Fst.by.contig$Hb.minus.Hw)/sum(Fst.by.contig$Hb) )
```

```{r bhatia-fst-by-contig-hist-FOLDED}
hist(Fst.by.contig$FST, xlab=expression(paste("average Bhatia's ", F[ST])), 
     main=bquote(paste(F[ST], "'s from ", .(nrow(Fst.by.contig)), " contigs")),
     col="grey"
     )
# proportion of loci with Fst greater than average Fst:
gtAvgFst = sum(Fst.by.contig$FST > Fst.bhatia.global)/length(Fst.by.contig$FST)
```

```{r bhatia-boot-global-Fst-FOLDED, cache=TRUE}
library(dplyr) # for 'sample_n'
library(parallel) # for 'mclapply'
#
boot = function(x){
  # creates bootstrap resamples of the rows in Fst.by.contig
  # and returns the global Fst from that
  rs = sample_n(Fst.by.contig, size=nrow(Fst.by.contig), replace=TRUE)
  sum(rs$Hb.minus.Hw)/sum(rs$Hb)
  }
#
# serial:
#boot.resample.Fst.by.contig = replicate(10000, boot())
# parallel:
startTime = proc.time()
boot.resample.Fst.by.contig = vector("double", length=10000)
boot.resample.Fst.by.contig= simplify2array(
  mclapply(1:length(boot.resample.Fst.by.contig),
          FUN=boot,
          mc.cores=20
           )
  )
stopTime = proc.time()
elapsedTime = stopTime - startTime
show(elapsedTime)
#
d = density(boot.resample.Fst.by.contig)
plot(d,
     xlab=expression(paste("global ", F[ST])),
     main=bquote(paste(.(length(boot.resample.Fst.by.contig)), 
                       " bootstrap resamples of ", .(nrow(Fst.by.contig)), 
                       " contigs"
                       )
                 )
     )
points(c(Fst.bhatia.global), c(0), pch=3, cex=1.5)
CI95 = quantile(boot.resample.Fst.by.contig, probs=c(.025, .975))
lines(d$x[d$x>CI95[1] & d$x<CI95[2]], 
      d$y[d$x>CI95[1] & d$x<CI95[2]], 
      type="h",
      col="grey")
legend("topright", legend=c("real sample", "95% CI"), pch=c(3,15), pt.cex=1.5, col=c("black", "grey"), bty="n")
```

### Reynolds

```{r}
Fst.reynolds.by.contig = aggregate(cbind(a, a.plus.b) ~ contig, 
                          data=reynolds, 
                          sum
                            )
Fst.reynolds.by.contig = cbind(Fst.reynolds.by.contig, FST=Fst.reynolds.by.contig[,2]/Fst.reynolds.by.contig[,3])
head(Fst.reynolds.by.contig)
( sum(Fst.reynolds.by.contig$a)/sum(Fst.reynolds.by.contig$a.plus.b) )
```


```{r reynolds-fst-by-contig-hist-FOLDED}
hist(Fst.reynolds.by.contig$FST, xlab=expression(paste("Reynolds ", F[ST])), 
     main=bquote(paste(F[ST], " over ", .(nrow(Fst.reynolds.by.contig)), " contigs")),
     col="grey"
     )
```

### 2D-SFS

```{r}
sfs2d = scan("/data3/claudius/Big_Data/ANGSD/FST/EryPar.FOLDED.2dsfs")
sfs2d = matrix(sfs2d, nrow=19, ncol=19) # rows should be PAR, columns should be ERY
sfs2d[1,1] = 0
```

```{r 2D-SFS-FOLDED}
library(fields)
# rows in the matrix are on the x-axis, columns are on the y-axis:
image.plot(0:19, 0:19, sfs2d, xlab="PAR", ylab="ERY", main="FOLDED 2D-SFS")
```


# References







