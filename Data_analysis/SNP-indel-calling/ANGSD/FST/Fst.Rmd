---
title: "Fst"
author: "Claudius"
date: "17/12/2016"
output: 
#  github_document:
#    toc: yes
  html_document:
    self_contained: true
    theme: cosmo
    toc: true
bibliography: Literature.bib
---

```{r setup options, cache=FALSE, include=FALSE}
library(knitr)
opts_chunk$set(dev=c("png", "pdf"))
setwd("/data3/claudius/Big_Data/ANGSD/FST")
```

# Unfolded SAF's

I have created unfolded SAF's and used an unfolded 2D-SFS to calculate posterior expectations of per-site $F_{ST}$. `realSFS` allows the calculation from two different formula's. One according to Reynolds [@Fumagalli2013] and the other accroding to Hudson/Bhatia [@Bhatia2013].

```{r, cache=TRUE}
if(!file.exists("bhatia.RData")){
  bhatia = read.delim("EryPar.Bhatia.fst.tab", header=F)
  names(bhatia) = c("contig", "pos", "Hb.minus.Hw", "Hb") 
  #nrow(bhatia)
  #str(bhatia)
  save(bhatia, file="bhatia.RData")
}else{
  load("bhatia.RData")
}
#
if(!file.exists("reynolds.RData")){
  reynolds = read.delim("EryPar.Reynolds.fst.tab", header=F)
  names(reynolds) = c("contig", "pos", "a", "a.plus.b")
  #head(reynolds)
  save(reynolds, file="reynolds.RData")
}else{
  load("reynolds.RData")
}
```

```{r}
head(bhatia)
head(reynolds)
```



## Global average $F_{ST}$ between ERY and PAR

From the per-site denominators and numerators of the two formulas, which are reported in columns 3 and 4 respectively, I want to calculate the global average $F_{ST}$ by taking the "ratio of averages" as suggested by [@Bhatia2013] (instead of the average of per-site $F_{ST}$ estimates).

```{r}
( Fst.bhatia.global = sum(bhatia[["Hb.minus.Hw"]])/sum(bhatia[["Hb"]]) )
nrow(bhatia)
( Fst.reynolds.global = sum(reynolds$a)/sum(reynolds[["a.plus.b"]]) )
nrow(reynolds)
```

Both formulas provide very similar estimates. The estimates are taken over `r nrow(bhatia)` sites. Reynolds $F_{ST}$ is weighting by sample size, which I think is an undesirable property (i. e. a change in sample size, that does not change the estimate of sample allele frequencies can change the estimate of $F_{ST}$).


## $F_{ST}$ by contig

In the following, I want to collapse the data set, by keeping only the average $F_{ST}$ per contig. 

### Bhatia

```{r}
Fst.by.contig = aggregate(cbind(Hb.minus.Hw, Hb) ~ contig, 
                          data=bhatia, 
                          sum
                            )
Fst.by.contig = cbind(Fst.by.contig, FST=Fst.by.contig[,2]/Fst.by.contig[,3])
#
contig.lengths = tapply(bhatia$contig, bhatia$contig, length)
head(names(contig.lengths))
head(Fst.by.contig)
tail(names(contig.lengths))
tail(Fst.by.contig)
Fst.by.contig = cbind(Fst.by.contig, length=contig.lengths)
head(Fst.by.contig)
save(Fst.by.contig, file="Fst.by.contig.bhatia.RData")
#
( sum(Fst.by.contig$Hb.minus.Hw)/sum(Fst.by.contig$Hb) )
```

I have now average $F_{ST}$ estimates for `r nrow(Fst.by.contig)` contigs.


```{r bhatia-fst-by-contig-hist-unfolded}
hist(Fst.by.contig$FST, xlab=expression(paste("average Bhatia's ", F[ST])), 
     breaks=seq(-0.05, 1, 0.01), # note, there are also some slightly negative Fst estimates
     main=bquote(paste(F[ST], "'s from ", .(nrow(Fst.by.contig)), " contigs")),
     col="black",
     # xlim=c(0, .3),
     border="black"
     )
# proportion of loci with Fst greater than average Fst:
gtAvgFst = sum(Fst.by.contig$FST > Fst.bhatia.global)/length(Fst.by.contig$FST)
```

As expected, the majority of the contigs are very little differentiated between the two populations, but `r signif(gtAvgFst*100, digits=1)`% of loci have $F_{ST}$ greater than the global average. Please read [@Whitlock2015]! It would be interesting to simulate neutral distributions of $F_{ST}$ under different demographic scenarios. The distribution might be overdispersed due to allele drop out [@Gautier2012].



#### Bootstrap 

It would be nice to get an estimate of uncertainty in the estimate of the global $F_{ST}$. There are two major sources of variation that influence this estimate:

1. sampling variation of individuals from populations
2. sampling variation of loci from the genome

In order to approximate the first source of variation, I could bootstrap resample individuals', but that requires many re-estimations of SAF's which will be quite compute and data intensive. I am therefore only going to approximate the second source of variation for the moment.

```{r bhatia-boot-global-Fst-unfolded, eval=FALSE}
# only needs to be run once

library(dplyr) # for 'sample_n'
library(parallel) # for 'mclapply'
#
boot = function(x){
  # creates bootstrap resamples of the rows in Fst.by.contig
  # and returns the global Fst from that
  rs = sample_n(Fst.by.contig, size=nrow(Fst.by.contig), replace=TRUE)
  sum(rs$Hb.minus.Hw)/sum(rs$Hb)
  }
#
# serial:
#boot.resample.Fst.by.contig = replicate(10000, boot())
# parallel:
startTime = proc.time()
boot.resample.Fst.by.contig = vector("double", length=10000)
boot.resample.Fst.by.contig = simplify2array(
  mclapply(1:length(boot.resample.Fst.by.contig),
          FUN=boot,
          mc.cores=20
           )
  )
stopTime = proc.time()
elapsedTime = stopTime - startTime
show(elapsedTime)
# save bootstrap resample for later:
save(boot.resample.Fst.by.contig, file="Bootstrap/boot.resample.Fst.by.contig.RData")
```

```{r}
load("Bootstrap/boot.resample.Fst.by.contig.RData")
#
d = density(boot.resample.Fst.by.contig)
plot(d,
     xlab=expression(paste("global ", F[ST])),
     main=bquote(paste(.(length(boot.resample.Fst.by.contig)), 
                       " bootstrap resamples of ", .(nrow(Fst.by.contig)), 
                       " contigs"
                       )
                 )
     )
points(c(Fst.bhatia.global), c(0), pch=3, cex=1.5)
CI95 = quantile(boot.resample.Fst.by.contig, probs=c(.025, .975))
lines(d$x[d$x>CI95[1] & d$x<CI95[2]], 
      d$y[d$x>CI95[1] & d$x<CI95[2]], 
      type="h",
      col="grey")
legend("topright", legend=c("real sample", "95% CI"), pch=c(3,15), pt.cex=1.5, col=c("black", "grey"), bty="n")
```


### Reynolds

```{r}
Fst.reynolds.by.contig = aggregate(cbind(a, a.plus.b) ~ contig, 
                          data=reynolds, 
                          sum
                            )
Fst.reynolds.by.contig = cbind(Fst.reynolds.by.contig, FST=Fst.reynolds.by.contig[,2]/Fst.reynolds.by.contig[,3])
head(Fst.reynolds.by.contig)
( sum(Fst.reynolds.by.contig$a)/sum(Fst.reynolds.by.contig$a.plus.b) )
```


```{r reynolds-fst-by-contig-hist-unfolded}
hist(Fst.reynolds.by.contig$FST, xlab=expression(paste("Reynolds ", F[ST])), 
     main=bquote(paste(F[ST], " over ", .(nrow(Fst.reynolds.by.contig)), " contigs")),
     col="grey"
     )
```


```{r}
boxplot(data.frame(Bhatia=Fst.by.contig$FST, Reynolds=Fst.reynolds.by.contig$FST), outline=FALSE, ylab=expression(F[ST]))
```

Reynolds' formula produces slightly higher $F_{ST}$ values.


### 2D-SFS

```{r}
sfs2d = scan("/data3/claudius/Big_Data/ANGSD/FST/EryPar.unfolded.2dsfs")
sfs2d = matrix(sfs2d, nrow=37, ncol=37) # rows should be PAR, columns should be ERY
sfs2d[1,1]
dim(sfs2d)
par_marginal_sfs = rowSums(sfs2d)
ery_marginal_sfs = colSums(sfs2d)
plot(1:(length(par_marginal_sfs)-1), par_marginal_sfs[2:length(par_marginal_sfs)], type="l")
sum(par_marginal_sfs)
# note, that the 'matrix is filled column-wise
sfs2d[1,1] = 0
# number of fixed differences:
fixed.diff = sfs2d[1,37] + sfs2d[37,1]
# proportion of fixed differences among polymorphic sites:
fixed.diff/sum(sfs2d)
```

```{r 2D-SFS-unfolded, fig.height=6, fig.width=7}
library(fields)
ticks = c(1, 10, 100, 500, 1000, 5000, 10000)
# rows in the matrix are on the x-axis, columns are on the y-axis:
image.plot(0:37, 0:37, log10(sfs2d+1), xlab="minor sample allele frequency in PAR", ylab="minor sample allele frequency in ERY", main="unfolded 2D-SFS",
           axis.args = list(at=log10(ticks), labels=ticks), legend.lab="number of SNP's",
           legend.line = 3
           )
```


## Permutation test of global $F_{ST}$

I have randomly permutated the 36 individuals into a population 1 and population 2. I have then estimated SAF's, 2D-SFS's and $F_{ST}$ for each permutation.


```{r, eval=FALSE}
# only needs to be run once:
fst.tabs = list.files("Bootstrap/", "*tab")
perm = function(x){
  bhatia = read.delim(fst.tabs[x], header=F)
  return( sum(bhatia[,3])/sum(bhatia[,4]) )
}
startTime = proc.time()
perm.fst = vector("double", len=length(fst.tabs))
perm.fst = simplify2array(
  mclapply(1:length(perm.fst),
          FUN=perm,
          mc.cores=20
           )
  )
stopTime = proc.time()
elapsedTime = stopTime - startTime
show(elapsedTime)
save(perm.fst, file="Bootstrapperm.global.fst.RData")
```

```{r permut-global-fst-hist}
load("Bootstrap/perm.global.fst.RData")
hist(boot.resample.Fst.by.contig, 
     border="navyblue", col="navyblue", 
     freq=FALSE, 
     xlim=c(0,.4),
     main="global average Fst\npermutation of individuals and bootstrapping of contigs",
     xlab=expression(paste("global Bhatia's ", F[ST]))
     )
hist(perm.fst, breaks=20, border="springgreen", col="springgreen", 
     add=TRUE,
     freq=FALSE
     )
legend("topleft", 
       legend=c("100 permutations of population label", 
                paste("10,000 bootstrap resamples of ", nrow(Fst.by.contig), " contigs")),
       col=c("springgreen", "navyblue"),
       cex=.7,
       fill=c("springgreen", "navyblue")
      )
```


## Jackknife correction of $F_{ST}$

The distribution of $F_{ST}$ from permutation of population labels has shown that there is a positive bias in the global $F_{ST}$ estimate of about `r round(median(perm.fst), 3)`. [@Weir1984], page 1366, propose a bias correction based on jackknife resampling loci.

```{r, eval=FALSE}
# only needs to be run once

library(parallel) # for 'mclapply'
#
jack = function(x){
  # creates delete-1 jackknife resample of the rows in Fst.by.contig
  # and returns the global Fst from that
  rs = Fst.by.contig[-x,]
  sum(rs$Hb.minus.Hw)/sum(rs$Hb)
  }
#
# serial:
#boot.resample.Fst.by.contig = replicate(10000, boot())
# parallel:
startTime = proc.time()
jack.resample.Fst.by.contig = vector("double", length=nrow(Fst.by.contig))
jack.resample.Fst.by.contig = simplify2array(
  mclapply(1:length(jack.resample.Fst.by.contig),
          FUN=jack,
          mc.cores=20
           )
  )
stopTime = proc.time()
elapsedTime = stopTime - startTime
show(elapsedTime)

# save jackknife resample for later:
save(jack.resample.Fst.by.contig, file="Bootstrap/jack.resample.Fst.by.contig.RData")
```


```{r}
load("Bootstrap/jack.resample.Fst.by.contig.RData")

# get bias corrected Fst:
n = nrow(Fst.by.contig)
( Fst.bhatia.global.bias.corrected = n*Fst.bhatia.global - (n-1)/n * sum(jack.resample.Fst.by.contig) )
Fst.bhatia.global - Fst.bhatia.global.bias.corrected
```

The jackknife applies only a negligible bias correction. 


## $F_{ST}$ by ascertainment class

I have calculated ML estimates of minor allele frequencies for each population, ERY and PAR. I have then created joint tables of each MAF file with `/data3/claudius/Big_Data/ANGSD/FST/EryPar.Bhatia.fst.tab`. I have now MAF's and $F_{ST}$ for 1,513,856 sites in ERY and
1,496,271 sites in PAR.

### Ascertainment in PAR

```{r}
pp = pipe("cut -f1,2,7,8,11,12 /data3/claudius/Big_Data/ANGSD/FST/MAFs/PAR/PAR.mafs.withFST", open="r")
par.mafs.withFST = read.delim(pp, header=F)
close(pp)
names(par.mafs.withFST) = c("contig", "pos", "MAF", "nind", "Hb.minus.Hw", "Hb")
head(par.mafs.withFST)
n = 18 # 18 individuals
global.fst.by.PAR.ascert = vector("double", n-1)
j = 0
for(i in 1:(n-1)){
  #print(i)
  j = j+1
  tab = par.mafs.withFST[par.mafs.withFST$MAF >= i/(2*n) & par.mafs.withFST$MAF < (i+1)/(2*n),]
  global.fst.by.PAR.ascert[j] = sum(tab["Hb.minus.Hw"])/sum(tab["Hb"])
}
save(global.fst.by.PAR.ascert, file="global.fst.by.PAR.ascert.RData")
```



### Ascertainment in ERY

```{r}
pp = pipe("cut -f1,2,7,8,11,12 /data3/claudius/Big_Data/ANGSD/FST/MAFs/ERY/ERY.mafs.withFST", open="r")
ery.mafs.withFST = read.delim(pp, header=F)
close(pp)
names(ery.mafs.withFST) = c("contig", "pos", "MAF", "nind", "Hb.minus.Hw", "Hb")
head(ery.mafs.withFST)
n = 18 # 18 individuals
global.fst.by.ERY.ascert = vector("double", n-1)
j = 0
for(i in 1:(n-1)){
  #print(i)
  j = j+1
  tab = ery.mafs.withFST[ery.mafs.withFST$MAF >= i/(2*n) & ery.mafs.withFST$MAF < (i+1)/(2*n),]
  global.fst.by.ERY.ascert[j] = sum(tab["Hb.minus.Hw"])/sum(tab["Hb"])
}
save(global.fst.by.ERY.ascert, file="global.fst.by.ERY.ascert.RData")
```

### Ascertainment in either ERY or PAR

```{r}
pp = pipe("cut -f1,2,7,8,11,12 /data3/claudius/Big_Data/ANGSD/FST/MAFs/EryPar/EryPar.mafs.withFST", open="r")
EryPar.mafs.withFST = read.delim(pp, header=F)
close(pp)
names(EryPar.mafs.withFST) = c("contig", "pos", "MAF", "nind", "Hb.minus.Hw", "Hb")
#head(ery.mafs.withFST)
n = 36 # 18 individuals
global.fst.by.EryPar.ascert = vector("double", n-1)
j = 0
for(i in 1:(n-1)){
  #print(i)
  j = j+1
  tab = EryPar.mafs.withFST[EryPar.mafs.withFST$MAF >= i/(2*n) & EryPar.mafs.withFST$MAF < (i+1)/(2*n),]
  global.fst.by.EryPar.ascert[j] = sum(tab["Hb.minus.Hw"])/sum(tab["Hb"])
}
save(global.fst.by.EryPar.ascert, file="global.fst.by.EryPar.ascert.RData")
```

I am trying to produce a plot analogous to figure 1 in [@Bhatia2013].

```{r global-Fst-by-ascertainment, fig.height=6}
n=18
plot(1:(n-1)/(2*n), global.fst.by.PAR.ascert, 
     ylim=c(0,.6), ylab=expression(paste("average Bhatia's ", F[ST])),
     xlab="minor allele frequency",
     pch=19, col="green",
     type="b",
     main="Allele frequency dependence of FST"
     )
lines(1:(n-1)/(2*n), global.fst.by.ERY.ascert, 
     pch=17, col="red",
     type="b"
     )
n=36
lines(1:(n-1)/(2*n), global.fst.by.EryPar.ascert, 
     pch=18, col="blue",
     type="b"
     )
legend("topleft",
       legend=c("ascertainment in PAR", "ascertainment in ERY", "ascertainment in ERY or PAR", "without ascertainment"),
       col=c("green", "red", "blue", "darkgrey"),
       pch=c(19, 17, 18, NA),
       lty=c(NA, NA, NA, 1),
       lwd=c(NA, NA, NA, 3),
       bty="n"
       )
abline(h=Fst.bhatia.global, lwd=2, col="darkgrey")
```

Now, I am going to add 95% bootstrap confidence intervals to these points.

```{r, eval=FALSE}
# get 95% bootstrap confidence intervals for the Fst by MAF in PAR

library(dplyr)
library(parallel)

# check that data frame is loaded
#head(par.mafs.withFST)

# take resample over sites (not contigs -> there is no MAF for a contig):
resample = function(x, tab){
  # creates bootstrap resamples of the rows in x
  # and returns the global Fst from that
  rs = sample_n(tab, size=nrow(tab), replace=TRUE)
  sum(rs$Hb.minus.Hw)/sum(rs$Hb)
  }
#
boot = function (tab){
  # get 10,000 bootstrap resamples of Fst
  boot.Fst.tab = vector("double", 10000)
  boot.Fst.tab = simplify2array(
  mclapply(X=1:length(boot.Fst.tab),
          FUN=resample, tab=tab,
          mc.cores=20
           )
  )
  return(boot.Fst.tab)
  }
#
n = 18
fst.by.PAR.ascert.bootQ = data.frame(
  low=vector("double", n-1), 
  med=vector("double", n-1), 
  high=vector("double", n-1)
  )
#
startTime = proc.time()
for(i in 1:(n-1)){
  tab = par.mafs.withFST[par.mafs.withFST$MAF >= i/(2*n) & par.mafs.withFST$MAF < (i+1)/(2*n), c("Hb.minus.Hw", "Hb")]
  fst.by.PAR.ascert.bootQ[i,] = quantile(boot(tab), probs=c(0.025, 0.5, 0.975))
}
stopTime = proc.time()
elapsedTime = stopTime - startTime
show(elapsedTime)
#
save(fst.by.PAR.ascert.bootQ, file="fst.by.PAR.ascert.bootQ.RData")
```

```{r, eval=FALSE}
# get 95% confidence intervals for the Fst by MAF in ERY

library(dplyr)
library(parallel)

# check that data frame is loaded
#head(ery.mafs.withFST)

# take resample over sites (not contigs -> there is no MAF for a contig):
resample = function(x, tab){
  # creates bootstrap resamples of the rows in x
  # and returns the global Fst from that
  rs = sample_n(tab, size=nrow(tab), replace=TRUE)
  sum(rs$Hb.minus.Hw)/sum(rs$Hb)
  }
#
boot = function (tab){
  # get 10,000 bootstrap resamples of Fst
  boot.Fst.tab = vector("double", 10000)
  boot.Fst.tab = simplify2array(
  mclapply(X=1:length(boot.Fst.tab),
          FUN=resample, tab=tab,
          mc.cores=20
           )
  )
  return(boot.Fst.tab)
  }
#
n = 18
fst.by.ERY.ascert.bootQ = data.frame(
  low=vector("double", n-1), 
  med=vector("double", n-1), 
  high=vector("double", n-1)
  )
#
startTime = proc.time()
for(i in 1:(n-1)){
  tab = ery.mafs.withFST[ery.mafs.withFST$MAF >= i/(2*n) & ery.mafs.withFST$MAF < (i+1)/(2*n), c("Hb.minus.Hw", "Hb")]
  fst.by.ERY.ascert.bootQ[i,] = quantile(boot(tab), probs=c(0.025, 0.5, 0.975))
}
stopTime = proc.time()
elapsedTime = stopTime - startTime
show(elapsedTime)
#
save(fst.by.ERY.ascert.bootQ, file="fst.by.ERY.ascert.bootQ.RData")
```

```{r, eval=FALSE}
# get 95% confidence intervals for the Fst by global MAF(across ERY and PAR) 

library(dplyr)
library(parallel)

# check that data frame is loaded
head(EryPar.mafs.withFST)

# take resample over sites (not contigs -> there is no MAF for a contig):
resample = function(x, tab){
  # creates bootstrap resamples of the rows in x
  # and returns the global Fst from that
  rs = sample_n(tab, size=nrow(tab), replace=TRUE)
  sum(rs$Hb.minus.Hw)/sum(rs$Hb)
  }
#
boot = function (tab){
  # get 10,000 bootstrap resamples of Fst
  boot.Fst.tab = vector("double", 10000)
  boot.Fst.tab = simplify2array(
  mclapply(X=1:length(boot.Fst.tab),
          FUN=resample, tab=tab,
          mc.cores=20
           )
  )
  return(boot.Fst.tab)
  }
#
n = 36
fst.by.EryPar.ascert.bootQ = data.frame(
  low=vector("double", n-1), 
  med=vector("double", n-1), 
  high=vector("double", n-1)
  )
#
startTime = proc.time()
for(i in 1:(n-1)){
  tab = EryPar.mafs.withFST[EryPar.mafs.withFST$MAF >= i/(2*n) & EryPar.mafs.withFST$MAF < (i+1)/(2*n), c("Hb.minus.Hw", "Hb")]
  fst.by.EryPar.ascert.bootQ[i,] = quantile(boot(tab), probs=c(0.025, 0.5, 0.975))
}
stopTime = proc.time()
elapsedTime = stopTime - startTime
show(elapsedTime)
#
save(fst.by.EryPar.ascert.bootQ, file="fst.by.EryPar.ascert.bootQ.RData")
```


```{r}
load("fst.by.PAR.ascert.bootQ.RData")
load("fst.by.ERY.ascert.bootQ.RData")
load("fst.by.EryPar.ascert.bootQ.RData")
#fst.by.PAR.ascert.bootQ
#fst.by.ERY.ascert.bootQ
#fst.by.EryPar.ascert.bootQ
```
```{r Fst-by-ascertainment-with-CI, fig.height=6}
n=36
plot(1:(n-1)/(2*n), fst.by.EryPar.ascert.bootQ$med, 
     ylim=c(0,.65), 
     ylab=expression(paste("average Bhatia's ", F[ST])),
     xlab="minor allele frequency",
     pch=18, col="blue",
     type="b",
     main="Allele frequency dependence of FST"
     )
arrows(1:(n-1)/(2*n), fst.by.EryPar.ascert.bootQ$med, 
       1:(n-1)/(2*n), fst.by.EryPar.ascert.bootQ$high,
       angle=90,
       length=.05,
       col="blue"
       )
arrows(1:(n-1)/(2*n), fst.by.EryPar.ascert.bootQ$med, 
       1:(n-1)/(2*n), fst.by.EryPar.ascert.bootQ$low,
       angle=90,
       length=.05,
       col="blue"
       )
#
#
#
n=18
lines(1:(n-1)/(2*n), fst.by.ERY.ascert.bootQ$med, 
     pch=17, col="red",
     type="b"
     )
arrows(1:(n-1)/(2*n), fst.by.ERY.ascert.bootQ$med, 
       1:(n-1)/(2*n), fst.by.ERY.ascert.bootQ$high,
       angle=90,
       length=.05,
       col="red"
       )
arrows(1:(n-1)/(2*n), fst.by.ERY.ascert.bootQ$med, 
       1:(n-1)/(2*n), fst.by.ERY.ascert.bootQ$low,
       angle=90,
       length=.05,
       col="red"
       )
#
#
#
lines(1:(n-1)/(2*n), fst.by.PAR.ascert.bootQ$med, 
     pch=19, col="green",
     type="b"
     )
arrows(1:(n-1)/(2*n), fst.by.PAR.ascert.bootQ$med, 
       1:(n-1)/(2*n), fst.by.PAR.ascert.bootQ$high,
       angle=90,
       length=.05,
       col="green"
       )
arrows(1:(n-1)/(2*n), fst.by.PAR.ascert.bootQ$med, 
       1:(n-1)/(2*n), fst.by.PAR.ascert.bootQ$low,
       angle=90,
       length=.05,
       col="green"
       )
#
#
#
legend("topleft",
       legend=c("ascertainment in PAR", "ascertainment in ERY", "ascertainment across ERY and PAR", "all sites"),
       col=c("green", "red", "blue", "darkgrey"),
       pch=c(19, 17, 18, NA),
       lty=c(NA, NA, NA, 1),
       lwd=c(NA, NA, NA, 3),
       bty="n"
       )
abline(h=Fst.bhatia.global, lwd=2, col="darkgrey")
```






## $F_{ST}$ distributions


```{r}
# ascertaining by ML estimate of minor allele frequency:
ery.snps = ery.mafs.withFST[ery.mafs.withFST$MAF>1/(2*n),]
par.snps = par.mafs.withFST[par.mafs.withFST$MAF>1/(2*n),] # not edible
nrow(ery.snps)
nrow(par.snps)
head(ery.snps, n=20)
```

Parallelus seems to have more SNP's than erythropus.

### Ascertained in _erythropus_

```{r fst-dist-ascert-in-ery-Hw}
plot(-1*(ery.snps$Hb.minus.Hw-ery.snps$Hb), ery.snps$Hb.minus.Hw/ery.snps$Hb,
     xlab=expression(H[w]),
     ylab=expression(1-H[w]/H[b]),
     pch=16,
     col=gray(level=0, alpha=.3),
     main=expression(paste("Hudson/Bhatia's ", F[ST]))
    )
text(x=.15, y=.9, labels=paste(nrow(ery.snps), " SNP's ascertained in erythropus"), pos=4)
```

```{r fst-dist-ascert-in-ery-Hb}
plot(ery.snps$Hb, ery.snps$Hb.minus.Hw/ery.snps$Hb,
     xlab=expression(H[b]),
     ylab=expression(1-H[w]/H[b]),
     pch=16,
     col=gray(level=0, alpha=.3),
     main=expression(paste("Hudson/Bhatia's ", F[ST]))
    )
text(x=.05, y=.9, labels=paste(nrow(ery.snps), " SNP's ascertained in erythropus"), pos=4)
```

```{r fst-dist-ascert-in-ery-MAF1-9-Hb, fig.height=10, fig.width=10}
jpeg("fst-dist-ascert-in-ery-MAF1-9-Hb.jpg", width=1000, height=1000, quality=100)
par(mfrow=c(3,3))
for(i in 1:9){
# ascertainment in ery, no singletons
ery.snps = ery.mafs.withFST[ery.mafs.withFST$MAF>i/(2*n),]
nrow(ery.snps)
#
plot(ery.snps$Hb, ery.snps$Hb.minus.Hw/ery.snps$Hb,
     xlab=expression(H[b]),
     ylab=expression(1-H[w]/H[b]),
     pch=16,
     col=gray(level=0, alpha=.3),
     main=paste("MAF > ", i)
    )
text(x=.05, y=.9, labels=paste(nrow(ery.snps), " SNP's ascertained in erythropus"), pos=4)
}
dev.off()
```

### Ascertained in _parallelus_

```{r fst-dist-ascert-in-par-Hw}
plot(-1*(par.snps$Hb.minus.Hw-par.snps$Hb), par.snps$Hb.minus.Hw/par.snps$Hb,
     xlab=expression(H[w]),
     ylab=expression(1-H[w]/H[b]),
     pch=16,
     col=gray(level=0, alpha=.3),
     main=expression(paste("Hudson/Bhatia's ", F[ST]))
    )
text(x=.15, y=.9, labels=paste(nrow(par.snps), " SNP's ascertained in parallelus"), pos=4)
```

```{r fst-dist-ascert-in-par-Hb}
plot(par.snps$Hb, par.snps$Hb.minus.Hw/par.snps$Hb,
     xlab=expression(H[b]),
     ylab=expression(1-H[w]/H[b]),
     pch=16,
     col=gray(level=0, alpha=.3),
     main=expression(paste("Hudson/Bhatia's ", F[ST]))
    )
text(x=.05, y=.9, labels=paste(nrow(par.snps), " SNP's ascertained in parallelus"), pos=4)
```

Not sure what to make of these.




# Folded SAF's

see this [ngsTools thread](https://groups.google.com/d/topic/ngstools-user/9Z4viLJ7NJA/discussion)

I have also estimated *folded* SAF's and a folded 2D-SFS and estimated per-site $F_{ST}$ from that.

```{r, cache=TRUE}
bhatia = read.delim("EryPar.FOLDED.Bhatia.fst.tab", header=F)
names(bhatia) = c("contig", "pos", "Hb.minus.Hw", "Hb") 
reynolds = read.delim("EryPar.FOLDED.Reynolds.fst.tab", header=F)
names(reynolds) = c("contig", "pos", "a", "a.plus.b")
```

```{r}
( Fst.bhatia.global = sum(bhatia[["Hb.minus.Hw"]])/sum(bhatia[["Hb"]]) )
nrow(bhatia)
( Fst.reynolds.global = sum(reynolds$a)/sum(reynolds[["a.plus.b"]]) )
nrow(reynolds)
```

```{r}
Fst.by.contig = aggregate(cbind(Hb.minus.Hw, Hb) ~ contig, 
                          data=bhatia, 
                          sum
                            )
Fst.by.contig = cbind(Fst.by.contig, FST=Fst.by.contig[,2]/Fst.by.contig[,3])
head(Fst.by.contig)

( sum(Fst.by.contig$Hb.minus.Hw)/sum(Fst.by.contig$Hb) )
```

```{r bhatia-fst-by-contig-hist-FOLDED}
hist(Fst.by.contig$FST, xlab=expression(paste("average Bhatia's ", F[ST])), 
     main=bquote(paste(F[ST], "'s from ", .(nrow(Fst.by.contig)), " contigs")),
     col="grey"
     )
# proportion of loci with Fst greater than average Fst:
gtAvgFst = sum(Fst.by.contig$FST > Fst.bhatia.global)/length(Fst.by.contig$FST)
```

```{r bhatia-boot-global-Fst-FOLDED, cache=TRUE}
library(dplyr) # for 'sample_n'
library(parallel) # for 'mclapply'
#
boot = function(x){
  # creates bootstrap resamples of the rows in Fst.by.contig
  # and returns the global Fst from that
  rs = sample_n(Fst.by.contig, size=nrow(Fst.by.contig), replace=TRUE)
  sum(rs$Hb.minus.Hw)/sum(rs$Hb)
  }
#
# serial:
#boot.resample.Fst.by.contig = replicate(10000, boot())
# parallel:
startTime = proc.time()
boot.resample.Fst.by.contig = vector("double", length=10000)
boot.resample.Fst.by.contig= simplify2array(
  mclapply(1:length(boot.resample.Fst.by.contig),
          FUN=boot,
          mc.cores=20
           )
  )
stopTime = proc.time()
elapsedTime = stopTime - startTime
show(elapsedTime)
#
d = density(boot.resample.Fst.by.contig)
plot(d,
     xlab=expression(paste("global ", F[ST])),
     main=bquote(paste(.(length(boot.resample.Fst.by.contig)), 
                       " bootstrap resamples of ", .(nrow(Fst.by.contig)), 
                       " contigs"
                       )
                 )
     )
points(c(Fst.bhatia.global), c(0), pch=3, cex=1.5)
CI95 = quantile(boot.resample.Fst.by.contig, probs=c(.025, .975))
lines(d$x[d$x>CI95[1] & d$x<CI95[2]], 
      d$y[d$x>CI95[1] & d$x<CI95[2]], 
      type="h",
      col="grey")
legend("topright", legend=c("real sample", "95% CI"), pch=c(3,15), pt.cex=1.5, col=c("black", "grey"), bty="n")
```

### Reynolds

```{r}
Fst.reynolds.by.contig = aggregate(cbind(a, a.plus.b) ~ contig, 
                          data=reynolds, 
                          sum
                            )
Fst.reynolds.by.contig = cbind(Fst.reynolds.by.contig, FST=Fst.reynolds.by.contig[,2]/Fst.reynolds.by.contig[,3])
head(Fst.reynolds.by.contig)
( sum(Fst.reynolds.by.contig$a)/sum(Fst.reynolds.by.contig$a.plus.b) )
```


```{r reynolds-fst-by-contig-hist-FOLDED}
hist(Fst.reynolds.by.contig$FST, xlab=expression(paste("Reynolds ", F[ST])), 
     main=bquote(paste(F[ST], " over ", .(nrow(Fst.reynolds.by.contig)), " contigs")),
     col="grey"
     )
```

### 2D-SFS

```{r}
sfs2d = scan("/data3/claudius/Big_Data/ANGSD/FST/EryPar.FOLDED.2dsfs")
sfs2d = matrix(sfs2d, nrow=19, ncol=19) # rows should be PAR, columns should be ERY
sfs2d[1,1] = 0
```

```{r 2D-SFS-FOLDED}
library(fields)
# rows in the matrix are on the x-axis, columns are on the y-axis:
image.plot(0:19, 0:19, sfs2d, xlab="PAR", ylab="ERY", main="FOLDED 2D-SFS")
```


# References







